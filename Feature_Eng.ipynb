{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Feature Engineering"
      ],
      "metadata": {
        "id": "msp_En5NN4Yc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. What is a parameter?\n"
      ],
      "metadata": {
        "id": "FfJaxn9-N-Yg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A parameter is an internal variable of a model that gets adjusted during training to minimize the error or loss function.\n",
        "In Linear Regression,\n",
        "the model is:\n",
        "\n",
        "y = w1x1 + w2x2 + b\n",
        "\n",
        "Here,\n",
        "ð‘¤\n",
        "1\n",
        ",\n",
        "ð‘¤\n",
        "2 are weights and\n",
        "b (bias) are parameters.\n",
        "They are learned from data by minimizing the difference between predicted and actual values."
      ],
      "metadata": {
        "id": "JWIZgEJCOA3l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. What is correlation?\n",
        "What does negative correlation mean?\n"
      ],
      "metadata": {
        "id": "1dGiv59WOqmD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Correlation is a statistical measure that shows how strongly two variables are related to each other.\n",
        "It tells us whether and how one variable changes when the other changes.\n",
        "The correlation coefficient (r) ranges between â€“1 and +1:\n",
        "\n",
        "| Type                             | Meaning                                          | Example                  |\n",
        "| -------------------------------- | ------------------------------------------------ | ------------------------ |\n",
        "| **Positive Correlation (r > 0)** | Both variables increase or decrease together     | Height & Weight          |\n",
        "| **Negative Correlation (r < 0)** | One variable increases while the other decreases | Price & Demand           |\n",
        "| **Zero Correlation (r = 0)**     | No relationship between variables                | Shoe size & Intelligence |\n",
        "\n",
        " - Negative correlation means when one variable increases, the other tends to decrease â€” they move in opposite directions.\n",
        "\n",
        " - Example:\n",
        "\n",
        " - When the price of a product increases, the demand usually decreases.\n",
        "â‡’ They are negatively correlated."
      ],
      "metadata": {
        "id": "L7GSc1rmOzhT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Define Machine Learning. What are the main components in Machine Learning?"
      ],
      "metadata": {
        "id": "AMKU3IDtPGrZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Machine Learning is a branch of Artificial Intelligence (AI) that enables computers to learn patterns from data and make decisions or predictions without being explicitly programmed.\n",
        "\n",
        "| Component         | Description                                                                     | Example                                          |\n",
        "| ----------------- | ------------------------------------------------------------------------------- | ------------------------------------------------ |\n",
        "| **1. Data**       | The raw information (features, labels) used for training and testing the model. | House prices, images, text, etc.                 |\n",
        "| **2. Model**      | The mathematical or statistical structure that learns the patterns from data.   | Linear Regression, Decision Tree, Neural Network |\n",
        "| **3. Features**   | The measurable properties or input variables of the data.                       | Size, number of rooms, location (for a house)    |\n",
        "| **4. Algorithm**  | The method or procedure used to train the model and update its parameters.      | Gradient Descent, Random Forest, SVM             |\n",
        "| **5. Training**   | The process of feeding data to the model so it can learn patterns.              | Model learns weights and biases                  |\n",
        "| **6. Evaluation** | Testing the modelâ€™s performance using unseen data (test set).                   | Accuracy, RMSE, Precision, Recall                |\n",
        "| **7. Prediction** | The modelâ€™s output or decision based on new input data.                         | Predicting house price for a new location        |\n"
      ],
      "metadata": {
        "id": "YN4ios5jPOKP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. How does loss value help in determining whether the model is good or not?"
      ],
      "metadata": {
        "id": "Wwh_a6tgPhHq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " - The loss value (or error) measures how far the modelâ€™s predictions are from the actual/true values.\n",
        "It helps us understand how well or poorly the model is performing during training.\n",
        "\n",
        " - How It Works:\n",
        "\n",
        "1.   The model makes predictions â†’ compares them with actual labels.\n",
        "\n",
        "\n",
        "\n",
        "2.   The loss function calculates the difference (error) between them.\n",
        "\n",
        "3.   The goal of training is to minimize this loss by adjusting model parameters.\n",
        "\n",
        " - Example\n",
        " If youâ€™re predicting house prices:\n",
        "\n",
        "Actual price = â‚¹10,00,000\n",
        "\n",
        "Predicted price = â‚¹9,50,000\n",
        "\n",
        "Error = â‚¹50,000 â†’ Loss function converts this into a numerical loss value.\n",
        "| Loss Value            | Meaning                                                                         |\n",
        "| --------------------- | ------------------------------------------------------------------------------- |\n",
        "| **High Loss**         | Predictions are far from actual values â†’ **Model is poor / undertrained**       |\n",
        "| **Low Loss**          | Predictions are close to actual values â†’ **Model is performing well**           |\n",
        "| **No Change in Loss** | Model has stopped learning â†’ might need better data, tuning, or a new algorithm |\n"
      ],
      "metadata": {
        "id": "NG8xRtbgPnyb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. What are continuous and categorical variables?"
      ],
      "metadata": {
        "id": "encJwa5MQE1J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In Machine Learning, features (inputs) are often categorized as continuous or categorical â€” this helps decide how data should be processed and which algorithms can be used.\n",
        "\n",
        "1. Continuous Variables\n",
        "\n",
        " - Definition:\n",
        "Continuous variables are numerical values that can take any value within a range.\n",
        "They are measurable and often come from real-world quantities.\n",
        "\n",
        " - Examples:\n",
        "\n",
        "    Height (e.g., 180.5 cm)\n",
        "\n",
        "    Weight (e.g., 75.2 kg)\n",
        "\n",
        "    Temperature (e.g., 36.6Â°C)\n",
        "\n",
        "    Salary, Age, Distance, etc.\n",
        "\n",
        " - Key Point:\n",
        "You can perform mathematical operations (addition, average, etc.) on them.\n",
        "\n",
        "2. Categorical Variables\n",
        "\n",
        " - Definition:\n",
        "Categorical variables represent distinct categories or labels â€” they describe qualities or groups, not quantities.\n",
        "\n",
        " - Examples:\n",
        "\n",
        "    Gender â†’ {Male, Female, Other}\n",
        "\n",
        "    Color â†’ {Red, Blue, Green}\n",
        "\n",
        "    City â†’ {Pune, Mumbai, Delhi}\n",
        "\n",
        "    Education Level â†’ {High School, Graduate, Postgraduate}\n",
        "\n",
        " - Key Point:\n",
        "They are non-numeric and are often encoded using techniques like Label Encoding or One-Hot Encoding before feeding into ML models."
      ],
      "metadata": {
        "id": "AJy6JNc5QJrX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. How do we handle categorical variables in Machine Learning? What are the common techniques?"
      ],
      "metadata": {
        "id": "3LqDrLcxQrJG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Most ML algorithms canâ€™t directly work with categorical (non-numeric) data, so we need to convert them into numerical form â€” this process is called encoding.\n",
        " - Common Techniques to Handle Categorical Variables:\n",
        "    1. Label Encoding\n",
        "\n",
        "        Converts each category into a unique numeric code (0, 1, 2, â€¦).\n",
        "\n",
        "        Useful when categories have natural order (ordinal data).\n",
        "\n",
        "        Use when: Categories are ordinal (ordered).\n",
        "\n",
        "    \n",
        "    2. One-Hot Encoding\n",
        "\n",
        "        Creates a new binary column for each category.\n",
        "\n",
        "        Each row has 1 in the column of its category and 0 elsewhere.\n",
        "\n",
        "        Use when: Categories are nominal (no order).\n",
        "\n",
        "    3. Ordinal Encoding\n",
        "\n",
        "        Similar to label encoding but used only for ordered categories like Low < Medium < High.\n",
        "    4. Target Encoding (Mean Encoding)\n",
        "\n",
        "        Replace each category with the mean of the target variable for that category.\n",
        "\n",
        "        Use when: Large number of categories (high cardinality).\n",
        "    5. Frequency Encoding\n",
        "\n",
        "        Replaces each category with how frequently it appears in the dataset.\n",
        "\n",
        "| Technique              | Best for              | Description             |\n",
        "| ---------------------- | --------------------- | ----------------------- |\n",
        "| **Label Encoding**     | Ordered categories    | Assigns integer codes   |\n",
        "| **One-Hot Encoding**   | Unordered categories  | Creates binary columns  |\n",
        "| **Ordinal Encoding**   | Ranked data           | Preserves order         |\n",
        "| **Target Encoding**    | High-cardinality data | Uses mean of target     |\n",
        "| **Frequency Encoding** | Many categories       | Uses category frequency |\n"
      ],
      "metadata": {
        "id": "hGWlct_eQ0Uq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. What do you mean by training and testing a dataset?"
      ],
      "metadata": {
        "id": "o1dQMkMHR5tY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Machine Learning models need to learn from data and then prove that they can make correct predictions on new, unseen data.\n",
        "To do this, we split the dataset into two parts: training and testing datasets.\n",
        "1. Training Dataset\n",
        "\n",
        " - Definition:\n",
        "The training dataset is the portion of data used to train the model â€” that is, to help it learn the patterns and relationships between features (inputs) and the target (output).\n",
        " - Purpose:\n",
        "To let the algorithm adjust its parameters (like weights in linear regression) to minimize the loss/error.\n",
        "2. Testing Dataset\n",
        "\n",
        " - Definition:\n",
        "The testing dataset is a separate portion of data used after training to check how well the model performs on unseen data.\n",
        "\n",
        " - Example:\n",
        "After the model learns, it is tested on new houses whose prices it hasnâ€™t seen before.\n",
        "\n",
        " - Purpose:\n",
        "To evaluate model performance and check generalization (whether it performs well on new data)."
      ],
      "metadata": {
        "id": "4z3vt32sSC5S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. What is sklearn.preprocessing?"
      ],
      "metadata": {
        "id": "c3EWgj2gSZZO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "sklearn.preprocessing is a module in Scikit-learn (sklearn) that provides tools to prepare and transform raw data into a suitable format for machine learning models.\n",
        "\n",
        "It helps ensure that features are scaled, encoded, or normalized correctly before training â€” which greatly affects model performance.\n",
        "\n",
        " - Why Preprocessing Is Important:\n",
        "\n",
        "Most ML algorithms work better when:\n",
        "\n",
        "All features are on a similar scale\n",
        "\n",
        "Categorical variables are converted into numbers\n",
        "\n",
        "Missing values and outliers are handled\n",
        "\n",
        "Data is normalized or standardized\n",
        "\n",
        "Thatâ€™s exactly what sklearn.preprocessing helps with.\n",
        "\n",
        " - Common Functions in sklearn.preprocessing:\n",
        " | Function / Class         | Purpose                                                    | Example                              |\n",
        "| ------------------------ | ---------------------------------------------------------- | ------------------------------------ |\n",
        "| **`StandardScaler`**     | Standardizes data (mean = 0, std = 1)                      | Used in regression, SVM, neural nets |\n",
        "| **`MinMaxScaler`**       | Scales data to a fixed range (e.g. 0â€“1)                    | Used in neural networks              |\n",
        "| **`LabelEncoder`**       | Converts categorical labels into numeric codes             | Gender â†’ Male:0, Female:1            |\n",
        "| **`OneHotEncoder`**      | Converts categories into binary columns                    | City â†’ Pune, Mumbai, Delhi           |\n",
        "| **`Binarizer`**          | Converts numerical values into 0 or 1 based on a threshold |                                      |\n",
        "| **`PolynomialFeatures`** | Generates new polynomial features (e.g. xÂ², xÂ³)            | Used in polynomial regression        |\n",
        "| **`Normalizer`**         | Scales feature vectors to unit norm                        | Useful in text or image data         |\n"
      ],
      "metadata": {
        "id": "ZbmbTyAHSgv2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. What is a Test set?"
      ],
      "metadata": {
        "id": "u-khuEB6Sx4o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A test set is the portion of a dataset that is used to evaluate a trained machine learning model.\n",
        "It contains unseen data that the model has not learned from during training.\n",
        " - Purpose:\n",
        "\n",
        "    To check how well the model generalizes to new, unseen data.\n",
        "\n",
        "    To estimate real-world performance (accuracy, precision, recall, RMSE, etc.).\n",
        "\n",
        "    To avoid overfitting, where a model performs well on training data but poorly on new data.\n",
        " - Example:\n",
        "\n",
        "Suppose you have 1000 samples of house data.\n",
        "You split them as:\n",
        "\n",
        "800 (80%) â†’ Training set â†’ used to train the model\n",
        "\n",
        "200 (20%) â†’ Test set â†’ used to test model accuracy\n",
        "\n",
        "After training, you give the model the 200 unseen houses to predict their prices and check how close it gets to the actual prices."
      ],
      "metadata": {
        "id": "NUUSlogpS18R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. How do we split data for model fitting (training and testing) in Python?\n",
        "How do you approach a Machine Learning problem?"
      ],
      "metadata": {
        "id": "IIRxUY38TFTC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In Python, especially using Scikit-learn, we commonly use train_test_split to divide the dataset into training and testing sets.\n",
        "\n",
        "Example :\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "\n",
        "   X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "\n",
        "Explanation:\n",
        "\n",
        "test_size=0.2 â†’ 20% of data goes to the test set, 80% to training\n",
        "\n",
        "random_state=42 â†’ ensures reproducibility (same split every time)\n",
        "\n",
        "X_train, y_train â†’ used for training the model\n",
        "\n",
        "X_test, y_test â†’ used for testing the model\n",
        "\n",
        " - Approach to a Machine Learning Problem\n",
        "\n",
        "When solving an ML problem, we generally follow these steps:\n",
        "\n",
        "Step 1: Understand the Problem\n",
        "\n",
        "Define the goal (prediction, classification, clustering, etc.)\n",
        "\n",
        "Identify the target variable (what to predict)\n",
        "\n",
        "Step 2: Collect & Explore Data\n",
        "\n",
        "Gather dataset(s)\n",
        "\n",
        "Perform Exploratory Data Analysis (EDA) â†’ check distributions, missing values, correlations\n",
        "\n",
        "Step 3: Preprocess Data\n",
        "\n",
        "Handle missing values\n",
        "\n",
        "Encode categorical variables (LabelEncoder, OneHotEncoder)\n",
        "\n",
        "Scale/normalize numerical features (StandardScaler, MinMaxScaler)\n",
        "\n",
        "Split data into training and testing sets\n",
        "\n",
        "Step 4: Choose a Model\n",
        "\n",
        "Decide which ML algorithm fits the problem\n",
        "\n",
        "Regression â†’ Linear, Random Forest\n",
        "\n",
        "Classification â†’ Logistic Regression, SVM, Decision Trees\n",
        "\n",
        "Clustering â†’ K-Means, DBSCAN\n",
        "\n",
        "Step 5: Train the Model\n",
        "\n",
        "Fit the model on the training data (model.fit(X_train, y_train))\n",
        "\n",
        "Step 6: Evaluate the Model\n",
        "\n",
        "Test the model on unseen test data (model.predict(X_test))\n",
        "\n",
        "Check metrics like accuracy, RMSE, precision, recall, F1-score\n",
        "\n",
        "Step 7: Tune Hyperparameters\n",
        "\n",
        "Use GridSearchCV / RandomizedSearchCV to find optimal parameters\n",
        "\n",
        "Step 8: Deploy / Predict\n",
        "\n",
        "Apply the trained model to new data for predictions\n",
        "\n",
        "Monitor performance over time"
      ],
      "metadata": {
        "id": "r5YGM7jKTK4j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "11. Why do we have to perform EDA before fitting a model to the data?"
      ],
      "metadata": {
        "id": "x_ariJ35UQsP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "EDA (Exploratory Data Analysis) is the process of analyzing and visualizing your dataset to understand its structure, patterns, and anomalies before building a machine learning model.\n",
        "\n",
        "Performing EDA is crucial because garbage in â†’ garbage out: if the data is messy or misleading, the model wonâ€™t learn correctly.\n",
        "\n",
        " - Reasons to Perform EDA:\n",
        "\n",
        "   - Understand Data Distribution\n",
        "\n",
        "   - Helps to know how features are spread (normal, skewed, or uniform)\n",
        "\n",
        "   - Example: Income distribution may be skewed â†’ may require transformation\n",
        "\n",
        " - Detect Missing Values\n",
        "\n",
        "   - Identify null or NaN values that need handling\n",
        "\n",
        "   - Example: Impute missing age values with mean/median\n",
        "\n",
        " - Detect Outliers\n",
        "\n",
        "   - Outliers can bias the model, especially regression models\n",
        "\n",
        "   - Example: A house with price â‚¹10 Crores in a dataset of â‚¹50Lâ€“â‚¹1Cr\n",
        "\n",
        " - Check Relationships Between Variables\n",
        "\n",
        "   - Understand correlation between features and target\n",
        "\n",
        "   - Example: Strong correlation between house size and price\n",
        "\n",
        " - Identify Categorical and Numerical Features\n",
        "\n",
        "   - Decide encoding and scaling methods before training\n",
        "\n",
        " - Feature Selection / Engineering Ideas\n",
        "\n",
        "   - Identify which features to keep, remove, or combine\n",
        "\n",
        "   - Example: Creating total_rooms = bedrooms + bathrooms\n",
        "\n",
        " - Prevent Model Bias\n",
        "   \n",
        "   - Detect imbalanced classes in classification problems\n",
        "\n",
        "   - Example: 95% of data = class A â†’ model may ignore minority class"
      ],
      "metadata": {
        "id": "CDYZmgidUR1e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "12. What is correlation?"
      ],
      "metadata": {
        "id": "oLJBYOx0VEi_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Correlation is a statistical measure that shows the strength and direction of a relationship between two variables.\n",
        "\n",
        "It tells us how one variable changes when the other changes.\n",
        "Correlation Coefficient (r):\n",
        "\n",
        "Ranges from â€“1 to +1\n",
        "\n",
        "Formula: r=â€‹Cov(X,Y) / ÏƒXâ€‹ÏƒY\n",
        "\n",
        "where Cov(X, Y) = covariance, Ïƒâ‚“, Ïƒáµ§ = standard deviationsâ€‹\n",
        "\n",
        "| Type                 | Meaning                                      | Example                  |\n",
        "| -------------------- | -------------------------------------------- | ------------------------ |\n",
        "| **Positive (r > 0)** | Both variables increase or decrease together | Height & Weight          |\n",
        "| **Negative (r < 0)** | One increases while the other decreases      | Price & Demand           |\n",
        "| **Zero (r = 0)**     | No relationship                              | Shoe size & Intelligence |\n",
        "\n",
        " - Correlation tells us the strength and direction of a relationship between two variables.\n",
        "\n",
        "    Positive â†’ move together\n",
        "\n",
        "    Negative â†’ move opposite\n",
        "\n",
        "    Zero â†’ no clear relationship\n",
        "    "
      ],
      "metadata": {
        "id": "xrxrdFAWVGpg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "13. What does negative correlation mean?"
      ],
      "metadata": {
        "id": "f66DVjN3VpAe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Negative correlation means that two variables move in opposite directions â€” when one increases, the other tends to decrease.\n",
        "\n",
        " - Correlation coefficient (r) is less than 0 (r < 0)\n",
        "\n",
        " - Strength of negative correlation is measured by how close r is to â€“1\n",
        "\n",
        "    r = â€“1 â†’ perfect negative correlation\n",
        "\n",
        "    r = 0 â†’ no correlation\n",
        "\n",
        "    r < 0 â†’ partial negative correlation\n",
        "\n",
        "  - Example:\n",
        "\n",
        "Price of a product â†‘ â†’ Demand â†“\n",
        "\n",
        "Number of hours studied â†‘ â†’ Number of errors in exam â†“"
      ],
      "metadata": {
        "id": "EYokDq0UVw0n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "14. How can you find correlation between variables in Python?"
      ],
      "metadata": {
        "id": "Zh5H_H6FWGH9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In Python, you can use pandas or NumPy to calculate correlation between variables. The most common method is Pearson correlation."
      ],
      "metadata": {
        "id": "m3OKKrWfWIu3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Sample dataset\n",
        "data = {\n",
        "    'Height': [150, 160, 170, 180, 190],\n",
        "    'Weight': [50, 60, 65, 75, 85],\n",
        "    'Age': [20, 25, 30, 35, 40]\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Compute correlation matrix\n",
        "correlation_matrix = df.corr()\n",
        "print(correlation_matrix)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PQ5hFwtAWNZH",
        "outputId": "e2c06719-10c8-469f-f154-739bd3b06c29"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "         Height   Weight      Age\n",
            "Height  1.00000  0.99485  1.00000\n",
            "Weight  0.99485  1.00000  0.99485\n",
            "Age     1.00000  0.99485  1.00000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " -  Using scipy.stats.pearsonr"
      ],
      "metadata": {
        "id": "BvktZF7FWUn3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import pearsonr\n",
        "\n",
        "# Correlation between Height and Weight\n",
        "corr, p_value = pearsonr(df['Height'], df['Weight'])\n",
        "print(\"Correlation:\", corr)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O7ji5-vmWVz-",
        "outputId": "63cf8ff6-ae0a-4cc9-c3cf-2262f1ad1dc7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Correlation: 0.9948497511671096\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "15. What is causation? Explain difference between correlation and causation with an example."
      ],
      "metadata": {
        "id": "ThhYQBCEWi_T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Causation (or causal relationship) occurs when one variable directly affects another â€” changes in the first variable cause changes in the second.\n",
        "\n",
        "Implies cause-and-effect\n",
        "\n",
        "Requires evidence or experiments to establish\n",
        "\n",
        "Correlation alone cannot prove causation\n",
        "\n",
        " - Difference Between Correlation and Causation\n",
        "| Aspect         | Correlation                                                        | Causation                                      |\n",
        "| -------------- | ------------------------------------------------------------------ | ---------------------------------------------- |\n",
        "| **Definition** | Measures the **relationship** or association between two variables | One variable **directly influences** the other |\n",
        "| **Direction**  | No direction implied                                               | Has a clear direction (cause â†’ effect)         |\n",
        "| **Proof**      | Can exist without any cause-effect                                 | Requires evidence or mechanism                 |\n",
        "| **Example**    | Ice cream sales â†‘ â†” Drowning cases â†‘                               | Smoking â†’ Lung cancer                          |\n",
        "\n",
        "Example to Understand:\n",
        "\n",
        " - Correlation Example:\n",
        "\n",
        "Ice cream sales â†‘ in summer\n",
        "\n",
        "Number of drowning cases â†‘ in summer\n",
        "\n",
        "Observation: Correlated, but ice cream does not cause drowning\n",
        "\n",
        "Reason: Both are affected by a third factor â€” hot weather\n",
        "\n",
        " - Causation Example:\n",
        "\n",
        "Smoking â†’ Lung cancer\n",
        "\n",
        "Evidence shows smoking directly increases risk of lung cancer\n"
      ],
      "metadata": {
        "id": "M5nxbLkwWo6G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "16. What is an Optimizer? What are different types of optimizers? Explain each with an example."
      ],
      "metadata": {
        "id": "ydFZwanqW5lN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In Machine Learning and Deep Learning, an optimizer is an algorithm used to update the modelâ€™s parameters (like weights and biases) in order to minimize the loss function.\n",
        "\n",
        "How Optimizers Work\n",
        "\n",
        "Calculate the gradient of the loss function with respect to the model parameters.\n",
        "\n",
        "Adjust parameters in the opposite direction of the gradient (to reduce loss).\n",
        "\n",
        "Repeat until the model converges (loss is minimized).\n",
        "\n",
        "Types of Optimizers\n",
        "\n",
        "1ï¸.  Gradient Descent (GD)\n",
        "\n",
        "Updates parameters by calculating gradients over the entire training dataset.\n",
        "\n",
        "Example: Training a linear regression model with all data points at once.\n",
        "\n",
        "2ï¸.  Stochastic Gradient Descent (SGD)\n",
        "\n",
        "Updates parameters after each training example instead of the full dataset.\n",
        "\n",
        "Faster and can escape local minima due to randomness.\n",
        "\n",
        "Example: Updating weights after each image in a neural network during training.\n",
        "\n",
        "3ï¸.  Mini-Batch Gradient Descent\n",
        "\n",
        "Compromise between GD and SGD: updates parameters using small batches of data.\n",
        "\n",
        "Commonly used in deep learning.\n",
        "\n",
        "\n",
        "Example: Batch size = 32 â†’ update weights after every 32 samples.\n",
        "\n",
        "4ï¸.  Adaptive Optimizers\n",
        "\n",
        "These optimizers adjust learning rate automatically for each parameter.\n",
        "\n",
        "| Optimizer                             | Description                                                                         | Example Use                         |\n",
        "| ------------------------------------- | ----------------------------------------------------------------------------------- | ----------------------------------- |\n",
        "| **Momentum**                          | Adds fraction of previous update to current update â†’ speeds up convergence          | Neural networks with high curvature |\n",
        "| **RMSprop**                           | Divides learning rate by running average of recent gradients â†’ prevents oscillation | RNNs                                |\n",
        "| **Adam (Adaptive Moment Estimation)** | Combines Momentum + RMSprop â†’ widely used                                           | Deep learning models (CNNs, RNNs)   |\n"
      ],
      "metadata": {
        "id": "88TLMBGhW_Vx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "17. What is sklearn.linear_model ?"
      ],
      "metadata": {
        "id": "Ql31lnkRX2hT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "sklearn.linear_model is a module in Scikit-learn that provides linear models for regression and classification.\n",
        "\n",
        "Key Features:\n",
        "\n",
        "Easy to use and interpret\n",
        "\n",
        "Fast and efficient for small to medium datasets\n",
        "\n",
        "Supports regularization (to prevent overfitting)\n",
        "\n",
        "Common Classes in sklearn.linear_model:\n",
        "| Class                  | Description                                                                      | Example Use                                    |\n",
        "| ---------------------- | -------------------------------------------------------------------------------- | ---------------------------------------------- |\n",
        "| **LinearRegression**   | Predicts **continuous output** using a linear equation                           | Predict house prices                           |\n",
        "| **LogisticRegression** | Predicts **binary or multi-class labels**                                        | Spam email detection                           |\n",
        "| **Ridge**              | Linear regression with **L2 regularization**                                     | Prevents overfitting                           |\n",
        "| **Lasso**              | Linear regression with **L1 regularization** (can shrink some coefficients to 0) | Feature selection                              |\n",
        "| **ElasticNet**         | Combines L1 + L2 regularization                                                  | When both sparsity & regularization are needed |\n"
      ],
      "metadata": {
        "id": "cOa6VG5HX4tz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "18. What does model.fit() do? What arguments must be given?\n"
      ],
      "metadata": {
        "id": "mclVbSaSYSu7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In Scikit-learn, the fit() method is used to train a machine learning model on a given dataset.\n",
        "\n",
        "How It Works:\n",
        "\n",
        "Takes training data (features) and target values as input.\n",
        "\n",
        "Uses the algorithm associated with the model (e.g., Linear Regression, Decision Tree)\n",
        "\n",
        "Finds the best parameters that minimize the loss/error.\n",
        "\n",
        "The model is now ready to predict new data using model.predict().\n",
        "\n",
        "Example:"
      ],
      "metadata": {
        "id": "VULVRfB8Yfwl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# Sample data\n",
        "X = [[1], [2], [3], [4], [5]]  # features\n",
        "y = [2, 4, 6, 8, 10]           # target\n",
        "\n",
        "# Create model\n",
        "model = LinearRegression()\n",
        "\n",
        "# Train model\n",
        "model.fit(X, y)  # X = features, y = target\n",
        "\n",
        "# Model has now \"learned\" the weights\n",
        "print(\"Coefficient:\", model.coef_)\n",
        "print(\"Intercept:\", model.intercept_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0z9LBwErZnfv",
        "outputId": "25b74e6a-8299-4cd0-dd9e-0d0dd8fdcb67"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Coefficient: [2.]\n",
            "Intercept: 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "19. What does model.predict() do? What arguments must be given?\n"
      ],
      "metadata": {
        "id": "Ae4lGYjWZskk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In Scikit-learn, the predict() method is used to make predictions using a trained machine learning model.\n",
        "\n",
        "How It Works:\n",
        "\n",
        "The model has already learned parameters from fit() (e.g., weights in Linear Regression).\n",
        "\n",
        "predict() applies the learned formula or rules to new data (X_new).\n",
        "\n",
        "Returns predictions for each input row in the same order.\n",
        "\n",
        "Example:"
      ],
      "metadata": {
        "id": "ULxddl_kZ2A0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# Training data\n",
        "X_train = [[1], [2], [3], [4], [5]]\n",
        "y_train = [2, 4, 6, 8, 10]\n",
        "\n",
        "# Train the model\n",
        "model = LinearRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# New data for prediction\n",
        "X_new = [[6], [7]]\n",
        "\n",
        "# Make predictions\n",
        "predictions = model.predict(X_new)\n",
        "print(predictions)  # Output: [12. 14.]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vALzOgT8Z_zG",
        "outputId": "19697f45-4045-482b-bbe1-ac8a4368a40e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[12. 14.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "20. What are continuous and categorical variables?"
      ],
      "metadata": {
        "id": "RMnOHD1_aI9F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In Machine Learning, features (inputs) are usually categorized as continuous or categorical. Knowing the type helps decide how to preprocess and model the data.\n",
        "\n",
        "1. Continuous Variables\n",
        "\n",
        "Definition:\n",
        "Continuous variables are numerical values that can take any value within a range.\n",
        "They are measurable and often come from real-world quantities.\n",
        "\n",
        "Examples:\n",
        "\n",
        "Height (e.g., 170.5 cm)\n",
        "\n",
        "Weight (e.g., 75.2 kg)\n",
        "\n",
        "Temperature (e.g., 36.6Â°C)\n",
        "\n",
        "Salary, Age, Distance\n",
        "\n",
        "Key Point:\n",
        "You can perform mathematical operations on them (mean, sum, etc.).\n",
        "\n",
        "2. Categorical Variables\n",
        "\n",
        "Definition:\n",
        "Categorical variables represent distinct categories or labels â€” they describe qualities or groups, not quantities.\n",
        "\n",
        "Examples:\n",
        "\n",
        "Gender â†’ {Male, Female, Other}\n",
        "\n",
        "Color â†’ {Red, Blue, Green}\n",
        "\n",
        "City â†’ {Pune, Mumbai, Delhi}\n",
        "\n",
        "Education Level â†’ {High School, Graduate, Postgraduate}\n",
        "\n",
        "Key Point:\n",
        "They are non-numeric and often need encoding (Label Encoding or One-Hot Encoding) before using in ML models."
      ],
      "metadata": {
        "id": "Jsr81KGIaTc3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "21. What is feature scaling? How does it help in Machine Learning?"
      ],
      "metadata": {
        "id": "1sKWqo8Yagg-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Feature Scaling is the process of normalizing or standardizing the range of independent variables (features) in a dataset so that they are on a similar scale.\n",
        "Why Feature Scaling is Needed:\n",
        "\n",
        "Features with different ranges can bias the model.\n",
        "\n",
        "Example: Salary (â‚¹50,000â€“â‚¹5,00,000) vs Age (20â€“60 years) â†’ Salary dominates in gradient calculation.\n",
        "\n",
        "Helps gradient-based algorithms (like Gradient Descent) converge faster.\n",
        "\n",
        "Improves performance of distance-based algorithms:\n",
        "\n",
        " - K-Nearest Neighbors (KNN)\n",
        "\n",
        " - K-Means Clustering\n",
        "\n",
        " - SVM\n",
        "\n",
        "Example in Python:"
      ],
      "metadata": {
        "id": "OeUSyxueahn4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "\n",
        "# Sample data\n",
        "X = [[10, 200], [15, 400], [20, 600]]\n",
        "\n",
        "# Standardization\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "print(X_scaled)\n",
        "\n",
        "# Min-Max Scaling\n",
        "scaler = MinMaxScaler()\n",
        "X_normalized = scaler.fit_transform(X)\n",
        "print(X_normalized)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XcrZt6bCa1_W",
        "outputId": "9417ae02-2c2e-4be9-d028-c6a3ab45f533"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[-1.22474487 -1.22474487]\n",
            " [ 0.          0.        ]\n",
            " [ 1.22474487  1.22474487]]\n",
            "[[0.  0. ]\n",
            " [0.5 0.5]\n",
            " [1.  1. ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "22. How do we perform scaling in Python?"
      ],
      "metadata": {
        "id": "0lnbv84pa9lW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Python, Scikit-learn (sklearn.preprocessing) provides built-in tools for scaling numerical features.\n",
        "1. Standardization (Z-score Scaling)\n",
        "\n",
        "Centers data to mean = 0, standard deviation = 1\n",
        "\n",
        "Useful for gradient-based algorithms like Linear Regression, Logistic Regression, Neural Networks\n",
        "\n",
        "2. Min-Max Scaling (Normalization)\n",
        "\n",
        "Scales values to a fixed range [0,1]\n",
        "\n",
        "Useful for distance-based algorithms like KNN or Neural Networks with sigmoid activation\n",
        "\n",
        "3. MaxAbs Scaling\n",
        "\n",
        "Scales each feature by its maximum absolute value â†’ range [-1,1]\n",
        "\n",
        "Useful for sparse data\n",
        "\n",
        "4. Robust Scaling\n",
        "\n",
        "Uses median and interquartile range (IQR) â†’ less sensitive to outliers"
      ],
      "metadata": {
        "id": "lAmHvC0sa-1M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Standardization (Z-score Scaling)\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Sample data\n",
        "X = [[10, 200], [15, 400], [20, 600]]\n",
        "\n",
        "# Create scaler\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Fit and transform data\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "print(X_scaled)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uXgd9eZgbTnR",
        "outputId": "65810ded-f486-4256-b9bb-9cbb45ebe394"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[-1.22474487 -1.22474487]\n",
            " [ 0.          0.        ]\n",
            " [ 1.22474487  1.22474487]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Min-Max Scaling (Normalization)\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "X_normalized = scaler.fit_transform(X)\n",
        "print(X_normalized)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ukRGsuKQbhOU",
        "outputId": "d195f807-6f9f-4411-8018-1e9c47384e78"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.  0. ]\n",
            " [0.5 0.5]\n",
            " [1.  1. ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6PMyJy2BMlbX",
        "outputId": "c403724a-9707-4172-8c4e-f76c85309e70"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.5        0.33333333]\n",
            " [0.75       0.66666667]\n",
            " [1.         1.        ]]\n"
          ]
        }
      ],
      "source": [
        "# 3. MaxAbs Scaling\n",
        "\n",
        "from sklearn.preprocessing import MaxAbsScaler\n",
        "\n",
        "scaler = MaxAbsScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "print(X_scaled)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Robust Scaling\n",
        "\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "\n",
        "scaler = RobustScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "print(X_scaled)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vdB6tQaAcHon",
        "outputId": "01ac3ff7-5637-4693-a642-3fe7a0abc865"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[-1. -1.]\n",
            " [ 0.  0.]\n",
            " [ 1.  1.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "23. What is sklearn.preprocessing?"
      ],
      "metadata": {
        "id": "ENYp5_zWcMWo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "sklearn.preprocessing is a module in Scikit-learn that provides tools for preparing and transforming data before feeding it into machine learning models.\n",
        "\n",
        "Why Preprocessing is Important\n",
        "\n",
        "Most ML algorithms cannot handle raw categorical or unscaled data\n",
        "\n",
        " - Preprocessing ensures:\n",
        "\n",
        "Features are on a similar scale\n",
        "\n",
        "Categorical data is converted to numerical form\n",
        "\n",
        "Outliers or skewed distributions are handled\n",
        "\n",
        "Model trains efficiently and converges faster\n",
        "\n",
        " - Common Functions and Classes in sklearn.preprocessing\n",
        "\n",
        "| Function / Class       | Purpose                                                    | Example                          |\n",
        "| ---------------------- | ---------------------------------------------------------- | -------------------------------- |\n",
        "| **StandardScaler**     | Standardizes features to mean=0, std=1                     | For regression, SVM, neural nets |\n",
        "| **MinMaxScaler**       | Scales features to a fixed range [0,1]                     | Useful in neural networks        |\n",
        "| **LabelEncoder**       | Converts categorical labels to numeric codes               | Gender â†’ Male:0, Female:1        |\n",
        "| **OneHotEncoder**      | Converts categorical variables into binary columns         | City â†’ Pune, Mumbai, Delhi       |\n",
        "| **Binarizer**          | Converts numerical values into 0 or 1 based on a threshold | Thresholding continuous features |\n",
        "| **PolynomialFeatures** | Generates polynomial features (xÂ², xÂ³, xâ‚xâ‚‚)               | Polynomial regression            |\n",
        "| **Normalizer**         | Scales feature vectors to unit norm                        | Text or image data               |\n",
        "\n",
        "\n",
        "Example: Standardization"
      ],
      "metadata": {
        "id": "j_e2boANcMNq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "X = [[10, 200], [15, 400], [20, 600]]\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "print(X_scaled)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DbQwdqwwcnWb",
        "outputId": "8761f41c-9fc1-44d5-dfbc-7b4a0eb8ddd5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[-1.22474487 -1.22474487]\n",
            " [ 0.          0.        ]\n",
            " [ 1.22474487  1.22474487]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "24. How do we split data for model fitting (training and testing) in Python?"
      ],
      "metadata": {
        "id": "eb1bmfRJcp0Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In Machine Learning, we split the dataset into training and testing sets to:\n",
        "\n",
        "Train the model on known data (training set)\n",
        "\n",
        "Evaluate its performance on unseen data (testing set)\n",
        "\n",
        "This ensures the model generalizes well.\n",
        "\n",
        "Using train_test_split from Scikit-learn"
      ],
      "metadata": {
        "id": "l5ZH_3zPcuSx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# X = features, y = target\n",
        "X = [[1], [2], [3], [4], [5]]\n",
        "y = [2, 4, 6, 8, 10]\n",
        "\n",
        "# Split data: 80% training, 20% testing\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "print(\"X_train:\", X_train)\n",
        "print(\"X_test:\", X_test)\n",
        "print(\"y_train:\", y_train)\n",
        "print(\"y_test:\", y_test)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rlNw3enpc-Jw",
        "outputId": "e3aa0c78-0675-4532-f95c-4c6afca6b667"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_train: [[5], [3], [1], [4]]\n",
            "X_test: [[2]]\n",
            "y_train: [10, 6, 2, 8]\n",
            "y_test: [4]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explanation of Parameters\n",
        "\n",
        "| Parameter      | Description                                             |\n",
        "| -------------- | ------------------------------------------------------- |\n",
        "| `X`            | Features / input variables                              |\n",
        "| `y`            | Target / output variable                                |\n",
        "| `test_size`    | Fraction of data for testing (e.g., 0.2 â†’ 20%)          |\n",
        "| `train_size`   | Fraction of data for training (optional)                |\n",
        "| `random_state` | Ensures reproducible splits                             |\n",
        "| `shuffle`      | Whether to shuffle data before splitting (default=True) |\n",
        "\n",
        "Typical Split Ratios\n",
        "\n",
        " - 80% train / 20% test â†’ most common\n",
        "\n",
        " - 70% train / 30% test â†’ sometimes used\n",
        "\n",
        " - 60% train / 20% validation / 20% test â†’ if using a separate validation set"
      ],
      "metadata": {
        "id": "X-JTGK73dBwS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "25. Explain data encoding?"
      ],
      "metadata": {
        "id": "S_0lPBPCdOCM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data encoding is the process of converting categorical (non-numeric) data into numerical form so that machine learning models can use it.\n",
        "\n",
        " - Why Encoding is Needed\n",
        "\n",
        "   - Machine learning models work with numbers, not text\n",
        "\n",
        "   - Allows models to understand relationships between categories\n",
        "\n",
        "   - Prevents errors when fitting models\n",
        "\n",
        " - Common Types of Data Encoding\n",
        "\n",
        "1ï¸. Label Encoding\n",
        "\n",
        "Converts each category into a unique integer code\n",
        "\n",
        "Useful for ordinal data (where order matters)\n",
        "\n",
        "2ï¸. One-Hot Encoding\n",
        "\n",
        "Converts each category into a binary column\n",
        "\n",
        "Useful for nominal data (no order)\n",
        "\n",
        "3ï¸. Ordinal Encoding\n",
        "\n",
        "Similar to Label Encoding but explicitly preserves order\n",
        "\n",
        "Example: Low < Medium < High\n",
        "\n",
        "4ï¸. Target / Mean Encoding\n",
        "\n",
        "Replace each category with the mean of the target variable\n",
        "\n",
        "Useful for high-cardinality data\n",
        "\n",
        " Must avoid data leakage"
      ],
      "metadata": {
        "id": "49x9q5ZudRZH"
      }
    }
  ]
}